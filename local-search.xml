<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>vscode remote SSH + frp</title>
    <link href="/2/"/>
    <url>/2/</url>
    
    <content type="html"><![CDATA[<h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>I heard that vscode remote SSH is now fully available. There has been an idea to configure the laboratory machine for remote programming, but teamviewer is a real card, and other methods are not as good as I want. But the sad thing is that the network architecture of the laboratory is NAT, and I don’t know how many levels of routing have been allocated to my machine. Naturally, my machine does not have a public IP, but I remember that my student discount is 10 yuan. The Alibaba Cloud host is still there every month, and there is a public IP, so let’s start the intranet penetration.</p><p>Update: Personally I feel a little tasteless, because I mainly use python to do more scientific calculations, it is better to use jupyter notebook. . . .</p><h3 id="Materials"><a href="#Materials" class="headerlink" title="Materials"></a>Materials</h3><ol><li>One laboratory machine, system: ubuntu 18.04 desktop 2. One cloud server, system: ubuntu 16.04 server 3. One mac.</li></ol><h3 id="Intranet-penetration"><a href="#Intranet-penetration" class="headerlink" title="Intranet penetration"></a>Intranet penetration</h3><p>The principle of intranet penetration is not introduced. I use <a href="https://github.com/fatedier/frp" target="_blank" rel="noopener">github’s open source project frp</a> . First download frp on two Ubuntu machines respectively</p><pre><code class="hljs sh">$ wget https://github.com/fatedier/frp/releases/download/v0.29.0/frp_0.29.0_freebsd_amd64.tar.gz</code></pre><p>After downloading the compressed package, unzip it, the name is too long, by the way, rename it</p><pre><code class="hljs sh">$ tar -zxvf frp_0.29.0_freebsd_amd64.tar.gz $ mv frp_0.29.0_freebsd_amd64 frp</code></pre><p>Then modify the server’s configuration file</p><pre><code class="hljs sh">$ <span class="hljs-built_in">cd</span> frp $ vi frps.ini</code></pre><p>The content is saved as follows:</p><pre><code class="hljs ini"><span class="hljs-comment"># frps.ini </span><span class="hljs-section">[common]</span> <span class="hljs-attr">bind_port</span> = <span class="hljs-number">7000</span></code></pre><p>Then run the frp server in the background</p><pre><code class="hljs sh">$ nohup ./frps -c ./frps.ini &amp;</code></pre><p>Then modify the configuration of the client (ie the laboratory machine), first enter the location where our client frp is decompressed</p><pre><code class="hljs sh">$ <span class="hljs-built_in">cd</span> frp</code></pre><p>Change setting</p><pre><code class="hljs sh">$ vi frpc.ini is</code></pre><p>The content is saved as follows</p><pre><code class="hljs ini"><span class="hljs-comment"># frpc.ini</span><span class="hljs-section">[common]</span><span class="hljs-attr">server_addr</span> = x.x.x.x<span class="hljs-attr">server_port</span> = <span class="hljs-number">7000</span><span class="hljs-section">[ssh]</span><span class="hljs-attr">type</span> = tcp<span class="hljs-attr">local_ip</span> = <span class="hljs-number">127.0</span>.<span class="hljs-number">0.1</span><span class="hljs-attr">local_port</span> = <span class="hljs-number">22</span><span class="hljs-attr">remote_port</span> = <span class="hljs-number">6000</span></code></pre><p>After the above configuration is completed, we need to configure Alibaba Cloud server security group rules to expose ports 7000 and 6000. See how this part of the operation <a href="https://help.aliyun.com/document_detail/25471.html" target="_blank" rel="noopener"> Ali cloud Related Documents </a> Note that the agreement to set up custom tcp, authorization object can be set to <code>0.0.0.0/0</code> server After the port is exposed, you can start the client’s intranet penetration program</p><pre><code class="hljs sh">$ nohup ./frpc -c ./frpc.ini &amp;</code></pre><p>After this operation, you can ssh to the laboratory machine on the mac with the following command</p><pre><code class="hljs sh">$ ssh -oPort=6000 username@server<span class="hljs-string">'s ip</span></code></pre><h3 id="Configure-ssh-password-free-login"><a href="#Configure-ssh-password-free-login" class="headerlink" title="Configure ssh password-free login"></a>Configure ssh password-free login</h3><p>After the above steps are completed, you can ssh to connect to the laboratory machine, but vscode remote ssh requires a public key to log in without password. First generate my public key on mac</p><pre><code class="hljs sh">$ ssh-keygen</code></pre><p>You need to enter some information. If you want to save trouble, just enter the box except the mailbox.</p><pre><code class="hljs sh">$ vi ~/.ssh/id_rsa.pub</code></pre><p>Copy everything inside, this is mac’s public key. Next, I need to find a way to copy the public key to the laboratory machine. I first placed it on the server, connected it to the server with the laboratory machine and then copied it, because the server also added a secret login to my mac.   (however, the security issue of using this public key is your own consideration, I think it is not a big problem)</p><p>Next on the laboratory machine</p><pre><code class="hljs sh">$ vi /etc/ssh/sshd_config</code></pre><p>Make sure to have the following lines</p><pre><code class="hljs ini">RSAAuthentication yesPubkeyAuthentication yesAuthorizedKeysFile  .ssh/authorized_keys</code></pre><p>If it is no, change to yes, if not, add</p><pre><code class="hljs sh">$ vi .ssh/authorized_keys</code></pre><p>Add the public key lab machine just copied from the mac and restart ssh</p><pre><code class="hljs sh">$ service sshd restart</code></pre><p>The following commandssh to the laboratory machine on mac without filling in the password</p><pre><code class="hljs sh">$ ssh -oPort=6000 username@server<span class="hljs-string">'s ip</span></code></pre><h3 id="vscode-remote-ssh-configuration"><a href="#vscode-remote-ssh-configuration" class="headerlink" title="vscode remote ssh configuration"></a>vscode remote ssh configuration</h3><p>After making sure that the mac can log in to the laboratory machine secretly, the next step is very simple, just refer to these two articles <a href="https://code.visualstudio.com/docs/remote/ssh" target="_blank" rel="noopener"> Official Document </a> <a href="https://zhuanlan.zhihu.com/p/64849549" target="_blank" rel="noopener"> 知知上Of </a></p><p>First go to the vscode plugin market to install the plugin:</p><p><img src="1.png" srcset="/img/loading.gif" alt=""></p><p>After installing the plugin, click <code>Add Host</code></p><p><img src="2.png" srcset="/img/loading.gif" alt=""></p><p>Enter host ssh address</p><p><img src="3.png" srcset="/img/loading.gif" alt=""></p><p>Waiting for the connection to complete, we can access the host at home to realize remote development.</p><p><img src="4.png" srcset="/img/loading.gif" alt=""></p><p><img src="5.png" srcset="/img/loading.gif" alt=""></p><h3 id="Experience"><a href="#Experience" class="headerlink" title="Experience"></a>Experience</h3><p>Personally, I feel that the design is very good. If it is only used in a local area network, it should be quite easy to use. However, the use of forced intranet penetration to achieve remote development. First, the bandwidth of the relay server limits the speed of SSH. The second is that I think we have a good rest when we rest, always thinking that remote development is really unnecessary.</p>]]></content>
    
    
    <categories>
      
      <category>Tech Talk</category>
      
    </categories>
    
    
    <tags>
      
      <tag>vscode frp</tag>
      
      <tag>ssh</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Talk about Apple WWDC 2020</title>
    <link href="/3/"/>
    <url>/3/</url>
    
    <content type="html"><![CDATA[<h1 id="Talk-about-Apple-WWDC-2020"><a href="#Talk-about-Apple-WWDC-2020" class="headerlink" title="Talk about Apple WWDC 2020"></a>Talk about Apple WWDC 2020</h1><p>Apple has released a lot of meaningful new features this time at WWDC. Whether it is for iOS, iPadOS or macOS, telling the truth is a big year. Compared with last year’s highlight of iOS, the fancy revision is not as good as it looks. This year’s new features are obviously more cross-epoch.</p><p>I am very happy to see that Apple has started to build its own unified framework for all platforms based on ARM. The integration of Apple’s ecological applications brought by this framework is believed to bring unpredictable superposition effects. In the current rich iOS ecosystem, this flattening action makes the benefits of macOS far greater than the previous Catalyst that requires developer access.</p><p>Looking back, we found that in the matter of ARM, Apple is far more radical than Microsoft. Windows has been trying to do ARM support since the early years of RT. After so many years of going down the road, even the latest Windows ARM version, support is still not ideal. Let’s not talk about the technical level for the time being. The two historical issues of compatibility and control over the ARM architecture, I think it is the reason that Windows has been unable to achieve good results in supporting ARM.</p><p>Today Apple can start turning to ARM and abandon Intel/AMD. The most basic point is that Apple’s highly customized ARM and the resulting system-level integration. This is like the x86 alliance that Microsoft and Intel established in the past. The difference is that this time Apple’s company’s volume and technology are sufficient to complete comprehensive support, and there is no need to hook up with other companies to co-create technology.</p><p>Apple’s conference today said that ARM is the future. In the past few years, due to x86 x64’s main Intel has been squeezing toothpaste, ARM has eaten up a large number of markets, and the development of mobile terminals and battery technology bottlenecks have made Intel The disadvantages in the field of personal devices gradually emerged, which ultimately led to the situation where ARM we see today occupy the mobile market and gradually expand.</p><p>However, ARM also has its own problems, such as the high degree of integration for mobile terminals, which makes it lack of customization and support of basic hardware interfaces compared to the x86 framework. The mainstream ARMs we can currently buy are highly integrated. What you need to change to a GPU is not to go out and buy a graphics card, but to need a FAB to help you customize the ARM chip you need, and then find a IDH. Starting from the motherboard to help you build a little platform, although more can be reused as you go up, the overall cost is still far from the convenient replacement of the graphics card under the x86 framework. This is also the reason why Taobao’s black apple store began to wailing after Apple announced today. In two years, this business can only be said to be a small sum of money.</p><p>But the future is full of infinite possibilities, just like the South Bridge and North Bridge that were born in the past, maybe ARM will also have its own customized solutions with the support of Apple, although according to Apple’s consistent style we finally saw the ARM machine It may be a complete customized machine, but there is always hope. Now that Apple and AMD are not so close, maybe a new PC era belonging to ARM is coming.</p><p>To pull it back, to say a little bit more, ARM has a big difference from the instruction set in terms of x86. In simple terms, ARM does very fine and broken things, x86 does very big and big things, and in a core clock Both of these CPUs can only do one thing. So no matter how powerful QEMU is, x86 running on ARM can still only get the efficiency of dividing the CPU core by 3 and dividing by 4. The efficiency of UTM on iOS and Qemu emulator on Android running Windows are both terrible. Whether Apple will try to solve some problems by extending the ARM instruction set this time is actually very exciting.</p><p>Linux and Docker were shown during Apple’s demonstration of compatibility this time. What about Windows? What about BootCamp? Now that Parallels is supported, Windows virtual machines are inevitable, but how efficient are they? In my opinion, if Apple’s compatibility strategy is a pure software layer, it will never deviate from Qemu’s thinking. If it is to be a hardware layer, the ARM Mac mini that can be bought today may not be reflected, after all, it is the A12Z core. It is likely that it is the same as the old iPad Pro, but it may also be that Apple hid it when it did the A12Z before and said that it is ready to today. Personally, I am not optimistic about the practice of emulating x86 on ARM. Although the ecosystem of macOS is about to be greatly enriched because of ARM, the necessity of Windows is still very high now. There is no Boot Camp and Windows virtual machine. How much is the value of macOS, I can only give a question mark.</p><p>The above balabala is a little messy, and it is all related to ARM. In fact, there are many things to talk about here, such as iOS/iPadOS, too late today, the elderly are going to sleep, and are free Say again.</p><p>Finally, a little regret about WWDC this time, that is AR. Throughout the speech, only one iPadOS mentioned ARKit upgrade to the 4th generation, and the rest was not mentioned. I believe there will definitely be a lot of press releases touting Apple’s abandonment of VR tomorrow. I can only regret this. It may be that the content of WWDC is too full this time, and I have no time to talk about AR. And for this kind of “Keynote movie”, if you show AR, it may be considered as a post-special effect, after all, a company Special effects financing is ahead.</p><p>I personally think that Apple may be staying in the big trick with AR. WWDC does not send hardware. Will the legendary Apple Glasses be released in September? Wait for me to download iOS 14 and explore it, maybe the answer is in it. It’s like the iPhone X Apple didn’t say anything when it was at WWDC, and finally gave a month to adapt after the press conference. Maybe this year’s Apple big move is still behind.</p>]]></content>
    
    
    <categories>
      
      <category>Tech Talk</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Apple</tag>
      
      <tag>Mac</tag>
      
      <tag>Ipad</tag>
      
      <tag>Airpods</tag>
      
      <tag>Iphone</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Duke University&#39;s open source AI algorithm makes mosaic pictures high-definition in seconds</title>
    <link href="/1/"/>
    <url>/1/</url>
    
    <content type="html"><![CDATA[<p>In this era of high-definition image quality, our tolerance for slag image quality is getting lower and lower.</p><p>When you search for “low resolution” and “slag quality” on Zhihu, you will see a large number of problems such as “how to remedy low-resolution photos” and “how to save slag quality”.</p><p>So, what kind of experience is it to change the screen from slag to mosaic level to high definition in seconds? Researchers at Duke University use AI algorithms to tell you.</p><p><img src="1.webp" srcset="/img/loading.gif" alt=""></p><h3 id="Unprecedented-“mosaic”-instantly-becomes-high-definition"><a href="#Unprecedented-“mosaic”-instantly-becomes-high-definition" class="headerlink" title="Unprecedented, “mosaic” instantly becomes high definition"></a>Unprecedented, “mosaic” instantly becomes high definition</h3><p>Researchers at Duke University have proposed an AI algorithm called  PULSE (Photo Upsampling via Latent Space Exploration, photo upsampling through potential space exploration).</p><p>The algorithm can convert blurred, unrecognizable face images into computer-generated images, the details of which are more detailed and realistic than ever before.</p><p><img src="2.gif" srcset="/img/loading.gif" alt=""></p><p>If you use the previous method and want to make a blurred “headshot” clear, you can only zoom this photo up to eight times the original resolution.</p><p>However, the team at Duke University has proposed a new method that can enlarge the 16x16 pixel Low Resolution (hereinafter referred to as LR) thumbnail in 64 seconds to  1024 x in just a few seconds. 1024-pixel high-resolution (High Resolution, HR) image .</p><p>Their AI tools will “imagine” some features that did not exist, even the details that could not be seen in the original LR photos, such as pores, fine lines, eyelashes, hair and stubble, etc., after being processed by their algorithm, they can See clearly.</p><p>Let’s look at a specific example:</p><p><img src="3.webp" srcset="/img/loading.gif" alt=""></p><p>Cynthia Rudin, a computer scientist at Duke University who led the team, said: “It has never been possible to create super-resolution images with a lot of detail using so few pixels like before.”</p><p>In terms of practical applications, Sachit Menon, co-author of the paper, said: “In these studies, we only used the face as a proof of concept.</p><p>But in theory, the technology is universal. From medicine and microscopy to astronomy and satellite imagery, the technology can improve image quality. “</p><h3 id="Break-the-traditional-operation-to-achieve-the-best-results"><a href="#Break-the-traditional-operation-to-achieve-the-best-results" class="headerlink" title="Break the traditional operation to achieve the best results"></a>Break the traditional operation to achieve the best results</h3><p>Although there have been many similar low-definition to high-definition methods before, it is the industry’s first to achieve a pixel magnification of 64 times.</p><blockquote><p>Traditional method: pixel matching, prone to bugs</p></blockquote><p>When dealing with such problems in the traditional way, generally after getting the LR image, you will “guess” how many extra pixels are needed, and then try to match the corresponding pixels in the previously processed HR image to the LR image.</p><p>The result of this simple matching of pixels is that pixels such as hair and skin textures will have mismatched pixels.</p><p>And this method will also ignore the perceptual details such as sensitivity in the HR image. So in the end there will be problems with smoothness and sensitivity, and the result will still look blurry or unreal.</p><p><img src="4.webp" srcset="/img/loading.gif" alt=""></p><blockquote><p>New method: low-definition image “Lianliankan”</p></blockquote><p>The new method proposed by the Duke University team can be said to open up new ideas.</p><p>After getting an LR image, the PULSE system will not slowly add new details, but traverse the HR images generated by the AI, compare the LR images corresponding to these HR images with the original image, and find the closest one.</p><p>An analogy is equivalent to taking the LR picture as a “Lianliankan” to find the most similar LR version, and then pushing it back. The HR image corresponding to this LR image is the final output.</p><p><img src="5.webp" srcset="/img/loading.gif" alt=""></p><p>The team used a generative adversarial network (GAN for short), which includes two neural networks trained on the same photo data set, namely a generator and a discriminator.</p><p>Among them, the generator simulates the face it has been trained to provide the face created by AI, and the discriminator obtains the output and determines whether it is enough to be fake.</p><p>With the accumulation of experience, the experience of the generator will get better and better until the discriminator cannot distinguish the difference.</p><p>They experimented with some real images, and the effect comparison is shown below:</p><p><img src="6.webp" srcset="/img/loading.gif" alt=""></p><p>Although there is still some gap between the generated high-resolution image and the original image, it is much clearer than the previous method.</p><h3 id="Evaluation-better-than-other-methods-score-close-to-real-photos"><a href="#Evaluation-better-than-other-methods-score-close-to-real-photos" class="headerlink" title="Evaluation: better than other methods, score close to real photos"></a>Evaluation: better than other methods, score close to real photos</h3><p>The team evaluated its algorithm on the famous high-resolution face dataset CelebA HQ, and conducted these experiments with scale factors of 64×, 32×, and 8×.</p><p>The researchers asked 40 people to rate 1,440 images generated by PULSE and five other zoom methods, with PULSE performing best, with scores almost as high as real high-quality photos.</p><p><img src="6.webp" srcset="/img/loading.gif" alt=""></p><p>Team members said that PULSE can create realistic images from noisy, low-quality input, even if the original image is not recognizable by eyes or mouth. This cannot be done by other methods.</p><p><img src="7.webp" srcset="/img/loading.gif" alt=""></p><p>However, the system cannot yet be used to identify identities, the researchers said: “It cannot turn out-of-focus, unrecognizable photos taken by a security camera into a clear image of a real person. It will only generate non-existent but seemingly real New face.”</p><p>In specific application scenarios, in addition to the above mentioned, the technology may be used in medicine and astronomy in the future. For the public, after having this black technology, you can change the old photos of N years ago to high definition. For editors and comrades, this is a great gospel, and you don’t have to look for high-resolution pictures anymore.</p><p>Reminder: The researchers will also introduce their methods at the ongoing CVPR 2020 (Computer Vision and Pattern Recognition Conference), you can pay attention to:</p><ul><li><p><a href="http://cvpr2020.thecvf.com/program/tutorials" target="_blank" rel="noopener">http://cvpr2020.thecvf.com/program/tutorials</a></p></li><li><p>Thesis address:<br><a href="https://arxiv.org/pdf/2003.03808.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2003.03808.pdf</a></p></li><li><p>Reference materials:<br><a href="https://www.sciencedaily.com/releases/2020/06/200612111409.htm" target="_blank" rel="noopener">https://www.sciencedaily.com/releases/2020/06/200612111409.htm</a></p></li></ul>]]></content>
    
    
    <categories>
      
      <category>Tech Talk</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>Computer vision</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
